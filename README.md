# Python-Lab
## –õ–∞–± 1
### –ó–∞–¥–∞–Ω–∏–µ 1

```
<<<<<<< HEAD
name=input (" –≤–∞—à–µ –∏–º—è:" )
Year= int(input( "–≤–∞—à –≤–æ–∑—Ä–∞—Å—Ç: "))
print(f" –ü—Ä–∏–≤–µ—Ç {name}!,  —á–µ—Ä–µ–∑ –≥–æ–¥ —Ç–µ–±–µ –±—É–¥–µ—Ç {Year+1}")
```
![](/images/lab01/img01.PNG)

### –ó–∞–¥–∞–Ω–∏–µ 2

```
n1=float(input())
n2=float(input())
sum=n1+n2
res=n1-n2
print(f"{sum:.1f} {res:.1f}")
```
![](/images/lab01/img02.PNG)

### –ó–∞–¥–∞–Ω–∏–µ 3

```
–°—Ç–æ–∏–º–æ—Å—Ç—å=float(input())
–°–∫–∏–¥–∫–∞=float(input())
–ù–î–°=float(input())
X=–°—Ç–æ–∏–º–æ—Å—Ç—å - (–°—Ç–æ–∏–º–æ—Å—Ç—å*–°–∫–∏–¥–∫–∞/100)
Y= X* –ù–î–°/100
total=X+Y
print(f"–¶–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–∫–∏–¥–∫–∏: ")
print(f"–ù–î–°: ")
print(f"–ò—Ç–æ–≥–æ –∫ –æ–ø–ª–∞—Ç–µ: {total:.2f}")
```
![](/images/lab01/img03.PNG)

### –ó–∞–¥–∞–Ω–∏–µ 4

```
Min=int(input())
Hour=Min//60
–æ—Å—Ç–∞—Ç–æ–∫=Min%60
print(f"{Hour}:{–æ—Å—Ç–∞—Ç–æ–∫:02d}")
```
![](/images/lab01/img04.PNG)

### –ó–∞–¥–∞–Ω–∏–µ 5

```
Name=input("–í–≤–µ–¥–∏—Ç–µ –§–ò–û: ")
inic=Name[0].upper()+"."
for i in range(len(Name)):
    if Name[i] == " " and i +1 < len(Name):
        inic = inic + Name [i + 1]. upper () + "."
lage= len (Name.replace(" ",""))
print (f"–§–ò–û:{Name}")
print (f"–ò–Ω–∏—Ü–∏–∞–ª—ã: {inic}")
print (f" –î–ª–∏–Ω–∞: {lage}")
```
![](/images/lab01/img05.PNG)

## –õ–∞–± 2
### –ó–∞–¥–∞–Ω–∏–µ 1

```
def min_max(nums: list[float | int]) -> tuple[float | int, float | int]:
    if not nums:
        raise ValueError("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç")

    minimum = min(nums)
    maximum = max(nums)
    return (minimum, maximum)


def unique_sorted(nums: list[float | int]) -> list[float | int]:
    unique_nums = set(nums)
    sorted_nums = sorted(unique_nums)
    return sorted_nums


def flatten(mat: list[list | tuple]) -> list:
    result_list = []
    for element in mat:
        if type(element) == list or type(element) == tuple:
            for sub_element in element:
                result_list.append(sub_element)
        elif type(element) == str:
            raise TypeError("—Å—Ç—Ä–æ–∫–∞ –Ω–µ —Å—Ç—Ä–æ–∫–∞ —Å—Ç—Ä–æ–∫ –º–∞—Ç—Ä–∏—Ü—ã")

        else:
            result_list.append(element)

    return result_list
```
![](/images/lab02/img01.PNG)
![](/images/lab02/img02.PNG)
![](/images/lab02/img03.PNG)

### –ó–∞–¥–∞–Ω–∏–µ 2

```
def transpose(mat):
    if not mat:
        return []

    row_length = len(mat[0])
    for row in mat:
        if len(row) != row_length:
            raise ValueError("–†–≤–∞–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞")

    return [list(row) for row in zip(*mat)]


def row_sums(mat):
    if not mat:
        return []

    row_length = len(mat[0])
    for row in mat:
        if len(row) != row_length:
            raise ValueError("–†–≤–∞–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞: —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã")

    sums = []
    for row in mat:
        row_sum = sum(row)
        sums.append(row_sum)

    return sums


def col_sums(mat: list[list[float | int]]) -> list[float]:
    if not mat:
        return []

    num_cols = len(mat[0])
    for row in mat:
        if len(row) != num_cols:
            raise ValueError("–†–≤–∞–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞")

    sums = [0] * num_cols
    for row in mat:
        for j in range(num_cols):
            sums[j] += row[j]

    return sums
```
![](/images/lab02/img04.PNG)
![](/images/lab02/img05.PNG)
![](/images/lab02/img06.PNG)

### –ó–∞–¥–∞–Ω–∏–µ 3

```
def format_record(rec: tuple[str, str, float]) -> str:
    fio, group, gpa = rec

    if not fio.strip():
        raise ValueError("–§–ò–û –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
    if not group.strip():
        raise ValueError("–ì—Ä—É–ø–ø–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π")
    if not isinstance(gpa, (float, int)):
        raise TypeError("GPA –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º")

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –§–ò–û –Ω–∞ —á–∞—Å—Ç–∏
    parts = fio.strip().split()
    parts = [part.strip() for part in parts if part.strip()]

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã
    if len(parts) >= 3:
        initials = (
            f"{parts[0].capitalize()} {parts[1][0].upper()}.{parts[2][0].upper()}."
        )
    elif len(parts) == 2:
        initials = f"{parts[0].capitalize()} {parts[1][0].upper()}."
    else:
        initials = parts[0].capitalize()  # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –§–ò–û –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º GPA —Å 2 –∑–Ω–∞–∫–∞–º–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
    gpa_formatted = f"{gpa:.2f}"

    return f"{initials}, –≥—Ä. {group}, GPA {gpa_formatted}"
```
![](/images/lab02/img07.PNG)
![](/images/lab02/img08.PNG)

## –õ–∞–± 3
### lib/text

```
import re


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace("—ë", "–µ").replace("–Å", "–ï")
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–≤–∏–¥–∏–º—ã–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    text = "".join(" " if c.isspace() else c for c in text)
    # –°—Ö–ª–æ–ø—ã–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω
    parts = text.split()
    normalized_text = " ".join(parts)
    return normalized_text


# –¢–µ—Å—Ç-–∫–µ–π—Å—ã:

# print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))  # "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
# print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))      # "–µ–∂–∏–∫, –µ–ª–∫–∞"
# print(normalize("Hello\r\nWorld"))  # "hello world"
# print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))  # "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"

def tokenize(text: str) -> list[str]:
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ —Å —É—á—ë—Ç–æ–º –¥–µ—Ñ–∏—Å–∞ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞
    pattern = r"\b\w+(?:-\w+)*\b"
    words = re.findall(pattern, text)
    return words


# print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))  # ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
# print(tokenize("hello,world!!!"))  # ["hello", "world"]
# print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))  # ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
# print(tokenize("2025 –≥–æ–¥"))  # ["2025", "–≥–æ–¥"]
# print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))  # ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]


def count_freq(tokens: list[str]) -> dict[str, int]:
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq


# print(count_freq(["a","b","a","c","b","a"]))  # {"a":3,"b":2,"c":1}
# print(count_freq(["bb","aa","bb","aa","cc"]))  # {"aa":2,"bb":2,"cc":1}


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    sorted_items = sorted(freq.items(), key=lambda item: (-item[1], item[0]))
    return sorted_items[:n]

# print(top_n({"a":3,"b":2,"c":1}, n=2))  # [("a",3), ("b",2)]
# print(top_n({'bb': 2, 'aa': 2, 'cc': 1}, n=2))  # [("a",3), ("b",2)]
```
![](/images/lab03/img01.PNG)
![](/images/lab03/img02.PNG)
![](/images/lab03/img03.PNG)
![](/images/lab03/img04.PNG)


### text_stats

```
from src.lib.text import normalize, tokenize, count_freq, top_n


def main():
    # –ß—Ç–µ–Ω–∏–µ –≤—Å–µ–≥–æ –≤–≤–æ–¥–∞ –¥–æ EOF
    input_text = input()
    # print("input_text: ", input_text)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    normalized_text = normalize(input_text)

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    tokens = tokenize(normalized_text)

    # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤
    freq = count_freq(tokens)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    total_words = len(tokens)
    unique_words = len(freq)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø-5 —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    top_5 = top_n(freq, n=5)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    TABLE_OUTPUT = False

    if not TABLE_OUTPUT:
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
        print("–¢–æ–ø-5:")
        for word, count in top_5:
            print(f"{word}:{count}")
    else:
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ä–µ–∂–∏–º–µ

        # –§–æ—Ä–º–∞—Ç:
        #
        # —Å–ª–æ–≤–æ        | —á–∞—Å—Ç–æ—Ç–∞
        # ----------------------
        # –ø—Ä–∏–≤–µ—Ç       | 10
        # –º–∏—Ä          | 7

        # –®–∏—Ä–∏–Ω–∞ —Å—Ç–æ–ª–±—Ü–∞ ¬´—Å–ª–æ–≤–æ¬ª ‚Äî –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–æ–ø–∞.

        if top_5:
            max_word_length = max(len(word) for word, count in top_5)
            print("\n–¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç:")
            print(f"{'—Å–ª–æ–≤–æ'.ljust(max_word_length)} | —á–∞—Å—Ç–æ—Ç–∞")
            print("-" * (max_word_length + 10))
            for word, count in top_5:
                print(f"{word.ljust(max_word_length)} | {count}")


if __name__ == "__main__":
    main()
```
![](/images/lab03/img05.PNG)

## –õ–∞–± 4
### io_txt_csv

```
import csv
from pathlib import Path


def read_text(path: str | Path, encoding: str = "utf-8") -> str:
    """
    –û—Ç–∫—Ä—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –Ω–∞ —á—Ç–µ–Ω–∏–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–∫ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É.
    –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É, –ø–µ—Ä–µ–¥–∞–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä encoding, –Ω–∞–ø—Ä–∏–º–µ—Ä: encoding="cp1251".
    """
    p = Path(path)
    with p.open(mode="r", encoding=encoding) as f:
        content = f.read()
    return content.replace("\n", " ")


# Example usage:
current_directory = Path(__file__).parent.parent
root_directory = current_directory.parent
# ok
# print(read_text(f"{root_directory}\\data\\samples\\text_example.txt", encoding="utf-8"))
# FileNotFoundError
# print(read_text(f"{root_directory}\\data\\samples\\text_not_found.txt", encoding="utf-8"))
# UnicodeDecodeError
# print(read_text(f"{root_directory}\\data\\samples\\text_example.txt", encoding="cp1251"))


def write_csv(
    rows: list[tuple | list], path: str | Path, header: tuple[str, ...] | None = None
) -> None:
    """
    –°–æ–∑–¥–∞—ë—Ç/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç CSV —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ,.
    –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω header, –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –µ–≥–æ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–æ–π.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ rows –∏–º–µ–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É (–∏–Ω–∞—á–µ ValueError).
    """
    p = Path(path)
    if header:
        expected_length = len(header)
    elif rows:
        expected_length = len(rows[0])
    else:
        expected_length = 0
    for row in rows:
        if len(row) != expected_length:
            raise ValueError("–í—Å–µ —Å—Ç—Ä–æ–∫–∏ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É")

    with p.open(mode="w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter=",")
        if header:
            writer.writerow(header)
        writer.writerows(rows)


# Example usage:
# ok
# write_csv([("word","count"),("test",3)], f"{root_directory}\\data\\out\\check.csv")
# ValueError
# write_csv([("word","count"),("test",3,4)], f"{root_directory}\\data\\out\\check.csv")
# edge case: –ø—É—Å—Ç—ã–º raws –° header=("a","b") ‚Üí —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫.
# write_csv([], f"{root_directory}\\data\\out\\check.csv", header=("a","b"))
# —Å –ø—É—Å—Ç—ã–º rows –∏ header=None ‚Üí —Å–æ–∑–¥–∞—ë—Ç—Å—è –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª (0 —Å—Ç—Ä–æ–∫).
write_csv([], f"{root_directory}\\data\\out\\check.csv")
```

### text_report

```
import sys
from pathlib import Path
import argparse
from src.lib.text import normalize, tokenize, count_freq, top_n
from src.lab04.io_txt_csv import read_text, write_csv


def main():
    parser = argparse.ArgumentParser(
        description="Generate word frequency report from text file."
    )
    parser.add_argument(
        "--in",
        dest="input_path",
        type=str,
        default="data/input.txt",
        help="Path to input text file",
    )
    parser.add_argument(
        "--out",
        dest="output_path",
        type=str,
        default="data/report.csv",
        help="Path to output CSV file",
    )
    parser.add_argument(
        "--encoding",
        dest="encoding",
        type=str,
        default="utf-8",
        help="File encoding (default: utf-8)",
    )
    args = parser.parse_args()

    input_path = Path(args.input_path)
    output_path = Path(args.output_path)
    encoding = args.encoding

    try:
        # Read input file
        input_text = read_text(input_path, encoding=encoding)

        # Normalize and tokenize text
        normalized_text = normalize(input_text)
        tokens = tokenize(normalized_text)

        # Count word frequencies
        freq = count_freq(tokens)

        # Prepare data for CSV
        rows = [(word, count) for word, count in freq.items()]
        rows.sort(key=lambda x: (-x[1], x[0]))  # Sort by count desc, word asc

        # Write to CSV
        write_csv(rows, output_path, header=("word", "count"))

        # Print summary
        total_words = len(tokens)
        unique_words = len(freq)
        top_5 = top_n(freq, n=5)

        print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
        print("–¢–æ–ø-5:")
        for word, count in top_5:
            print(f"{word}:{count}")

    except FileNotFoundError:
        print(f"Error: Input file '{input_path}' not found.")
        sys.exit(1)
    except UnicodeDecodeError:
        print(f"Error: Cannot decode file '{input_path}' with encoding '{encoding}'.")
        sys.exit(1)


if __name__ == "__main__":
    main()
```
![](/images/lab04/img01.PNG)
![](/images/lab04/img02.PNG)
![](/images/lab04/img03.PNG)
![](/images/lab04/img04.PNG)
![](/images/lab04/img05.PNG)
![](/images/lab04/img06.PNG)
![](/images/lab04/img07.PNG)
![](/images/lab04/img08.PNG)
![](/images/lab04/img09.PNG)
![](/images/lab04/img010.PNG)
![](/images/lab04/img011.PNG)

## –õ–∞–± 5
### json_cvs

```

import json
import csv
from pathlib import Path


def json_to_csv(json_path: str, csv_path: str) -> None:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç JSON-—Ñ–∞–π–ª –≤ CSV.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π [{...}, {...}], –∑–∞–ø–æ–ª–Ω—è–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏.
    –ö–æ–¥–∏—Ä–æ–≤–∫–∞ UTF-8. –ü–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ ‚Äî –∫–∞–∫ –≤ –ø–µ—Ä–≤–æ–º –æ–±—ä–µ–∫—Ç–µ –∏–ª–∏ –∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π (—É–∫–∞–∑–∞—Ç—å –≤ README).
    –Ω–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞, –ø—É—Å—Ç–æ–π JSON –∏–ª–∏ CSV ‚Üí ValueError.
    –æ—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª ‚Üí FileNotFoundError
    """

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    json_file = Path(json_path)
    if not json_file.is_file():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_path}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    if json_file.suffix.lower() != ".json":
        raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è JSON —Ñ–∞–π–ª")

    # –ß—Ç–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–∞
    with json_file.open(encoding="utf-8") as f:
        data = json.load(f)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π JSON –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    if (
        not isinstance(data, list)
        or not data
        or not all(isinstance(item, dict) for item in data)
    ):
        raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –∏–ª–∏ –ø—É—Å—Ç–æ–π JSON")

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–∫–ª—é—á–µ–π)
    headers = set()
    for item in data:
        headers.update(item.keys())
    headers = list(headers)

    # –ó–∞–ø–∏—Å—å –≤ CSV-—Ñ–∞–π–ª
    with open(csv_path, mode="w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for item in data:
            writer.writerow({key: item.get(key, "") for key in headers})


def csv_to_json(csv_path: str, json_path: str) -> None:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç CSV –≤ JSON (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π).
    –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏.
    json.dump(..., ensure_ascii=False, indent=2)
    """

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    csv_file = Path(csv_path)
    if not csv_file.is_file():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    if csv_file.suffix.lower() != ".csv":
        raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è CSV —Ñ–∞–π–ª")

    # –ß—Ç–µ–Ω–∏–µ CSV-—Ñ–∞–π–ª–∞
    with open(csv_path, mode="r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        data = list(reader)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π CSV –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if not data:
        raise ValueError("–ü—É—Å—Ç–æ–π CSV —Ñ–∞–π–ª –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫")

    # –ó–∞–ø–∏—Å—å –≤ JSON-—Ñ–∞–π–ª
    with open(json_path, mode="w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
current_directory = Path(__file__).parent.parent
root_directory = current_directory.parent

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
# ok
# json_to_csv(f'{root_directory}\\data\\samples\\people.json', f'{root_directory}\\data\\out\\people_from_json.csv')
# ValueError
# json_to_csv(f'{root_directory}\\data\\samples\\people.csv', f'{root_directory}\\data\\out\\people_from_json.csv')
# FileNotFoundError
# json_to_csv(f'{root_directory}\\data\\samples\\not_existing_file.json', f'{root_directory}\\data\\out\\people_from_json.csv')
# ok
# csv_to_json(f'{root_directory}\\data\\samples\\people.csv',
# f'{root_directory}\\data\\out\\people_from_csv.json')
# ValueError
# csv_to_json(f'{root_directory}\\data\\samples\\people.json',
#             f'{root_directory}\\data\\out\\people_from_csv.json')
# FileNotFoundError
# csv_to_json(f'{root_directory}\\data\\samples\\not_existing_file.csv',
#             f'{root_directory}\\data\\out\\people_from_csv.json')
```
![](/images/lab05/img01.PNG)
![](/images/lab05/img01_1.PNG)
![](/images/lab05/img02.PNG)
![](/images/lab05/img03.PNG)
![](/images/lab05/img04.PNG)
![](/images/lab05/img04_1.PNG)
![](/images/lab05/img05.PNG)

### cvs_xlsx

```
from openpyxl import Workbook
import csv
from pathlib import Path


def csv_to_xlsx(csv_path: str, xlsx_path: str) -> None:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç CSV –≤ XLSX.
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å openpyxl –ò–õ–ò xlsxwriter.
    –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ CSV ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫.
    –õ–∏—Å—Ç –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è "Sheet1".
    –ö–æ–ª–æ–Ω–∫–∏ ‚Äî –∞–≤—Ç–æ—à–∏—Ä–∏–Ω–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ (–Ω–µ –º–µ–Ω–µ–µ 8 —Å–∏–º–≤–æ–ª–æ–≤).
    """

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    csv_file = Path(csv_path)
    if not csv_file.is_file():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    if csv_file.suffix.lower() != ".csv":
        raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è CSV —Ñ–∞–π–ª")

    # –ß—Ç–µ–Ω–∏–µ CSV-—Ñ–∞–π–ª–∞ –∏ –∑–∞–ø–∏—Å—å –≤ XLSX
    wb = Workbook()
    ws = wb.active
    ws.title = "Sheet1"

    with open(csv_path, mode="r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            ws.append(row)

    # –ê–≤—Ç–æ—à–∏—Ä–∏–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
    for col in ws.columns:
        max_length = 8  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞
        column = col[0].column_letter  # –ü–æ–ª—É—á–∞–µ–º –±—É–∫–≤—É –∫–æ–ª–æ–Ω–∫–∏
        for cell in col:
            try:
                if cell.value:
                    max_length = max(max_length, len(str(cell.value)))
            except:
                pass
        adjusted_width = max_length + 2
        ws.column_dimensions[column].width = adjusted_width

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ XLSX-—Ñ–∞–π–ª–∞
    wb.save(xlsx_path)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
current_directory = Path(__file__).parent.parent
root_directory = current_directory.parent

# ok
# csv_to_xlsx(f'{root_directory}\\data\\samples\\people.csv',
#             f'{root_directory}\\data\\out\\people_from_csv.xlsx')
# ValueError
# csv_to_xlsx(f'{root_directory}\\data\\samples\\people.json',
# f'{root_directory}\\data\\out\\people_from_csv.xlsx')
# FileNotFoundError
# csv_to_xlsx(f"{root_directory}\\data\\samples\\not_existed.csv",
# f"{root_directory}\\data\\out\\people_from_csv.xlsx")
```
![](/images/lab05/img06.PNG)
![](/images/lab05/img07.PNG)
![](/images/lab05/img08.PNG)

## –õ–∞–± 6
### cli_convert

```
import argparse
from src.lab05.json_csv import json_to_csv, csv_to_json
from src.lab05.cvs_xlsx import csv_to_xlsx


def main():
    parser = argparse.ArgumentParser(description="–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö")
    sub = parser.add_subparsers(dest="cmd")

    p1 = sub.add_parser("json2csv", description="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è JSON –≤ CSV")
    p1.add_argument("--in", dest="input", required=True, help="–í—Ö–æ–¥–Ω–æ–π JSON —Ñ–∞–π–ª")
    p1.add_argument("--out", dest="output", required=True, help="–í—ã—Ö–æ–¥–Ω–æ–π CSV —Ñ–∞–π–ª")

    p2 = sub.add_parser("csv2json", description="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è CSV –≤ JSON")
    p2.add_argument("--in", dest="input", required=True, help="–í—Ö–æ–¥–Ω–æ–π CSV —Ñ–∞–π–ª")
    p2.add_argument("--out", dest="output", required=True, help="–í—ã—Ö–æ–¥–Ω–æ–π JSON —Ñ–∞–π–ª")

    p3 = sub.add_parser("csv2xlsx", description="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è CSV –≤ XLSX")
    p3.add_argument("--in", dest="input", required=True, help="–í—Ö–æ–¥–Ω–æ–π CSV —Ñ–∞–π–ª")
    p3.add_argument("--out", dest="output", required=True, help="–í—ã—Ö–æ–¥–Ω–æ–π XLSX —Ñ–∞–π–ª")

    args = parser.parse_args()

    """
        –í—ã–∑—ã–≤–∞–µ–º –∫–æ–¥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤.
    """
    if args.cmd == "json2csv":
        json_to_csv(args.input, args.output)
    elif args.cmd == "csv2json":
        csv_to_json(args.input, args.output)
    elif args.cmd == "csv2xlsx":
        csv_to_xlsx(args.input, args.output)


if __name__ == "__main__":
    main()
```

### cli_text

```
import argparse


def main():
    parser = argparse.ArgumentParser(description="CLI‚Äë—É—Ç–∏–ª–∏—Ç–∞ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π ‚Ññ6")
    subparsers = parser.add_subparsers(dest="command")

    # –ø–æ–¥–∫–æ–º–∞–Ω–¥–∞ cat
    cat_parser = subparsers.add_parser("cat", description="–í—ã–≤–µ—Å—Ç–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞")
    cat_parser.add_argument("--input", required=True, help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É")
    cat_parser.add_argument("-n", action="store_true", help="–ù—É–º–µ—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏")

    # –ø–æ–¥–∫–æ–º–∞–Ω–¥–∞ stats
    stats_parser = subparsers.add_parser("stats", description="–ß–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤")
    stats_parser.add_argument("--input", required=True, help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É")
    stats_parser.add_argument(
        "--top", type=int, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø‚Äë—Å–ª–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)"
    )

    args = parser.parse_args()

    if args.command == "cat":
        # cat --input <path> [-n] ‚Äî –≤—ã–≤–æ–¥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (—Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π –ø—Ä–∏ -n).
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥—ã cat"""
        with open(args.input, "r", encoding="utf-8") as f:
            for i, line in enumerate(f, start=1):
                if args.n:
                    print(f"{i}\t{line.rstrip()}")
                else:
                    print(line.rstrip())
    elif args.command == "stats":
        # stats --input <txt> [--top 5] ‚Äî –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ src.lib.text).;
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥—ã stats"""
        from src.lib.text import normalize, tokenize, count_freq, top_n

        with open(args.input, "r", encoding="utf-8") as f:
            text = f.read()
        normalized_text = normalize(text)
        tokens = tokenize(normalized_text)
        frequencies = count_freq(tokens)

        if args.top:
            top_words = top_n(frequencies, n=args.top)
            for word, count in top_words:
                print(f"{word}:{count}")
        else:
            print(frequencies)


if __name__ == "__main__":
    main()
```
![](/images/lab06/img01.PNG)
![](/images/lab06/img02.PNG)
![](/images/lab06/img03.PNG)
![](/images/lab06/img04.PNG)

## –õ–∞–± 7
### test_json_csv

```
import json
from pathlib import Path

from src.lab05.json_csv import json_to_csv, csv_to_json

current_directory_path = Path(__file__).parent.parent
root_directory_path = current_directory_path.parent.parent
samples_directory_path = f"{root_directory_path}/data/samples"


def test_json_to_csv_ok(tmp_path):
    people_json_path = f"{samples_directory_path}/people.json"
    output_csv_path = f"{tmp_path}/people.csv"
    json_to_csv(people_json_path, output_csv_path)
    with open(people_json_path) as people_json_file:
        people_json = json.load(people_json_file)  # list of dicts
    with open(output_csv_path) as output_csv_file:
        output_csv_list = output_csv_file.readlines()
    header_line_str = output_csv_list[0].rstrip("\n\r")
    splitted_headers = header_line_str.split(",")
    headers_set = set(splitted_headers)
    json_keys_set = set(people_json[0].keys())
    assert headers_set == json_keys_set
    assert len(output_csv_list) == len(people_json) + 1  # +1 for header


def test_json_to_csv_file_not_found():
    not_existing_json_path = f"{samples_directory_path}/not_existing_file.json"
    output_csv_path = "output.csv"
    try:
        json_to_csv(not_existing_json_path, output_csv_path)
    except FileNotFoundError as e:
        assert str(e) == f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {not_existing_json_path}"
    else:
        assert False, "Expected FileNotFoundError was not raised"


def test_json_to_csv_input_file_not_json(tmp_path):
    invalid_json_path = f"{samples_directory_path}/people.csv"
    output_csv_path = f"{tmp_path}/output.csv"
    try:
        json_to_csv(invalid_json_path, output_csv_path)
    except ValueError as e:
        assert str(e) == "–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è JSON —Ñ–∞–π–ª"
    else:
        assert False, "Expected ValueError was not raised"


def test_csv_to_json_ok(tmp_path):
    people_csv_path = f"{samples_directory_path}/people.csv"
    output_json_path = f"{tmp_path}/people.json"
    csv_to_json(people_csv_path, output_json_path)
    with open(people_csv_path) as people_csv_file:
        people_csv = people_csv_file.readlines()
    with open(output_json_path) as output_json_file:
        output_json = json.load(output_json_file)  # list of dicts
    header_line_str = people_csv[0].rstrip("\n\r")
    splitted_headers = header_line_str.split(",")
    headers_set = set(splitted_headers)
    json_keys_set = set(output_json[0].keys())
    assert headers_set == json_keys_set
    assert len(output_json) == len(people_csv) - 1  # -1 for header


def test_csv_to_json_file_not_found():
    not_existing_csv_path = f"{samples_directory_path}/not_existing_file.csv"
    output_json_path = "output.json"
    try:
        csv_to_json(not_existing_csv_path, output_json_path)
    except FileNotFoundError as e:
        assert str(e) == f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {not_existing_csv_path}"
    else:
        assert False, "Expected FileNotFoundError was not raised"


# –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –Ω–µ csv —Ñ–æ—Ä–º–∞—Ç–∞ ‚Üí –æ–∂–∏–¥–∞–µ–º ValueError;
def test_csv_to_json_input_file_not_csv(tmp_path):
    invalid_csv_path = f"{samples_directory_path}/people.json"
    output_json_path = f"{tmp_path}/output.json"
    try:
        csv_to_json(invalid_csv_path, output_json_path)
    except ValueError as e:
        assert str(e) == "–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è CSV —Ñ–∞–π–ª"
    else:
        assert False, "Expected ValueError was not raised"
```

### test_text

```
import pytest

from src.lib.text import normalize, tokenize, count_freq, top_n


@pytest.mark.parametrize(
    "not_normalized_text, expected_normalized_text",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
        ("Hello\r\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),
        ("#", "#"),
    ],
)
def test_normalize_ok(not_normalized_text, expected_normalized_text):
    assert normalize(not_normalized_text) == expected_normalized_text


@pytest.mark.parametrize(
    "actual_text, expected_tokens",
    [
        ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("hello,world!!!", ["hello", "world"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
        ("", []),
    ],
)
def test_tokenize_ok(actual_text, expected_tokens):
    assert tokenize(actual_text) == expected_tokens


@pytest.mark.parametrize(
    "tokens, expected_freq",
    [
        (["a", "b", "a", "c", "b", "a"], {"a": 3, "b": 2, "c": 1}),
        (["bb", "aa", "bb", "aa", "cc"], {"aa": 2, "bb": 2, "cc": 1}),
        ([], {}),
    ],
)
def test_count_freq_ok(tokens, expected_freq):
    assert count_freq(tokens) == expected_freq


@pytest.mark.parametrize(
    "freq, n, expected_top_n",
    [
        ({"a": 3, "b": 2, "c": 1}, 2, [("a", 3), ("b", 2)]),
        ({"bb": 2, "aa": 2, "cc": 1}, 2, [("aa", 2), ("bb", 2)]),
        ({"aa": 2, "bb": 2, "cc": 1}, 2, [("aa", 2), ("bb", 2)]),
    ],
)
def test_top_n_ok(freq, n, expected_top_n):
    assert top_n(freq, n) == expected_top_n
```
![](/images/lab07/img01.PNG)
![](/images/lab07/img02.PNG)
![](/images/lab07/img03.PNG)

## –õ–∞–± 8
### models

```
import re
from dataclasses import dataclass
from datetime import datetime, date
from typing import ClassVar


@dataclass
class Student:
    fio: str
    birthdate: str
    group: str
    gpa: float

    DATE_FORMAT: ClassVar[str] = "%Y-%m-%d"
    DATE_REGEX: ClassVar[re.Pattern] = re.compile(r"^\d{4}-\d{2}-\d{2}$")

    def __post_init__(self):
        # Validate birthdate format
        if not self.DATE_REGEX.match(self.birthdate):
            raise ValueError(f"Invalid date format for birthdate: {self.birthdate}. Expected YYYY-MM-DD.")
        try:
            datetime.strptime(self.birthdate, self.DATE_FORMAT)
        except ValueError:
            raise ValueError(f"Invalid date value for birthdate: {self.birthdate}.")

        # Validate gpa range
        if not (0 <= self.gpa <= 5):
            raise ValueError(f"GPA must be between 0 and 5. Given: {self.gpa}")

    def age(self) -> int:
        birth_date = datetime.strptime(self.birthdate, self.DATE_FORMAT).date()
        today = date.today()
        age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day))
        return age

    def to_dict(self) -> dict:
        return {
            "fio": self.fio,
            "birthdate": self.birthdate,
            "group": self.group,
            "gpa": self.gpa,
        }

    @classmethod
    def from_dict(cls, data: dict) -> 'Student':
        return cls(
            fio=data["fio"],
            birthdate=data["birthdate"],
            group=data["group"],
            gpa=data["gpa"],
        )

    def __str__(self) -> str:
        return f"{self.fio}, gr. {self.group}, GPA {self.gpa:.1f}"
```

### serialize

```
import json
from pathlib import Path
from typing import List
from src.lab08.models import Student


def students_to_json(students: List[Student], path: str) -> None:
    data = [s.to_dict() for s in students]
    with open(path, mode="w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def students_from_json(path: str) -> List[Student]:
    with open(path, mode="r", encoding="utf-8") as f:
        data = json.load(f)
    students = [Student.from_dict(item) for item in data]
    return students

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
current_directory = Path(__file__).parent.parent
root_directory = current_directory.parent

students = [
    Student(fio="–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á", birthdate="2000-12-15", group="SE-01", gpa=4.5),
    Student(fio="–ü–µ—Ç—Ä–æ–≤ –ü–µ—Ç—Ä –ü–µ—Ç—Ä–æ–≤–∏—á", birthdate="1999-05-20", group="SE-02", gpa=3.8),
]
students_to_json(students, f"{root_directory}\\data\\lab08\\students_output.json")

loaded_students = students_from_json(f"{root_directory}\\data\\lab08\\students_input.json")
for student in loaded_students:
    print(student)
```
![](/images/lab08/img01.PNG)
![](/images/lab08/img02.PNG)
![](/images/lab08/img03.PNG)
=======
Name= input("–≤–∞—à–µ –∏–º—è:" )
Year= int(input("–≤–∞—à –≤–æ–∑—Ä–∞—Å—Ç:"))
print(f"–ü—Ä–∏–≤–µ—Ç {Name}!, —á–µ—Ä–µ–∑ –≥–æ–¥ —Ç–µ–±–µ –±—É–¥–µ—Ç {Year+1}")
```
<![alt text](01.1.png)>

### –ó–∞–¥–∞–Ω–∏–µ 2

```
n1=float(input("–ù–æ–º–µ—Ä 1 ="))
n2=float(input("–ù–æ–º–µ—Ä 2 ="))
sum=n1+n2
res=n1-n2
print(f"C–ª–æ–∂–µ–Ω–∏–µ={sum:.1f}, –≤—ã—á–∏—Ç–∞–Ω–∏–µ={res:.1f}")
```

### –ó–∞–¥–∞–Ω–∏–µ 3

```
–¶–µ–Ω–∞=float(input("–¶–µ–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∞:"))
–°–∫–∏–¥–∫–∞=float(input("–°–∫–∏–¥–∫–∞:"))
–ù–î–°=float(input("–ù–î–°:"))
X= –¶–µ–Ω–∞ - (–¶–µ–Ω–∞*–°–∫–∏–¥–∫–∞/100)
Y= X* –ù–î–°/100
total=X+Y
print(f"–¶–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–∫–∏–¥–∫–∏: {X}")
print(f"–ù–î–°: {Y}")
print(f"–ò—Ç–æ–≥–æ –∫ –æ–ø–ª–∞—Ç–µ: {total:.2f}")
```

### –ó–∞–¥–∞–Ω–∏–µ 4

```
–ú–∏–Ω—É—Ç—ã=int(input("–ú–∏–Ω—É—Ç—ã: "))
–ß–∞—Å—ã=Min//60
–æ—Å—Ç–∞—Ç–æ–∫=Min%60
print(f"{–ß–∞—Å—ã}:{–æ—Å—Ç–∞—Ç–æ–∫:02d}")
```

### –ó–∞–¥–∞–Ω–∏–µ 5

```
Name=input("–í–≤–µ–¥–∏—Ç–µ –§–ò–û: ")
inic=Name[0].upper()+"."
for i in range(len(Name)):
    if Name[i] == " " and i +1 < len(Name):
        inic = inic + Name [i + 1]. upper () + "."
lage= len (Name.replace(" ",""))
print (f"–§–ò–û:{Name}")
print (f"–ò–Ω–∏—Ü–∏–∞–ª—ã: {inic}")
print (f"–î–ª–∏–Ω–∞: {lage}")
```
>>>>>>> a4cae7d5639e2a27fbf34d0d92eb8218d24fdfd2
